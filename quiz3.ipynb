{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e58c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../data/sales.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f05c5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+-----+------------+-----------------+--------------------+--------+--------------+----------------+----------------+-----------+----------+\n",
      "|Transaction_date| Product|Price|Payment_Type|             Name|                City|   State|       Country| Account_Created|      Last_Login|   Latitude| Longitude|\n",
      "+----------------+--------+-----+------------+-----------------+--------------------+--------+--------------+----------------+----------------+-----------+----------+\n",
      "| 01/02/2009 6:17|Product1| 1200|  Mastercard|         carolina|            Basildon| England|United Kingdom| 01/02/2009 6:00| 01/02/2009 6:08|       51.5|-1.1166667|\n",
      "| 01/02/2009 4:53|Product1| 1200|        Visa|           Betina|Parkville        ...|      MO| United States| 01/02/2009 4:42| 01/02/2009 7:49|     39.195| -94.68194|\n",
      "|01/02/2009 13:08|Product1| 1200|  Mastercard|Federica e Andrea|Astoria          ...|      OR| United States|01/01/2009 16:21|01/03/2009 12:32|   46.18806|   -123.83|\n",
      "|01/03/2009 14:44|Product1| 1200|        Visa|            Gouya|              Echuca|Victoria|     Australia|   9/25/05 21:13|01/03/2009 14:22|-36.1333333|    144.75|\n",
      "|01/04/2009 12:56|Product2| 3600|        Visa|          Gerd W |Cahaba Heights   ...|      AL| United States|  11/15/08 15:47|01/04/2009 12:45|   33.52056|  -86.8025|\n",
      "+----------------+--------+-----+------------+-----------------+--------------------+--------+--------------+----------------+----------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ee097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      Country|\n",
      "+-------------+\n",
      "|       Sweden|\n",
      "|       Jersey|\n",
      "|     Malaysia|\n",
      "|       Turkey|\n",
      "|      Germany|\n",
      "|       France|\n",
      "|      Belgium|\n",
      "|      Finland|\n",
      "|United States|\n",
      "|        India|\n",
      "|       Kuwait|\n",
      "|        Malta|\n",
      "|        Italy|\n",
      "|       Norway|\n",
      "|        Spain|\n",
      "|      Denmark|\n",
      "|      Ireland|\n",
      "|       Israel|\n",
      "|      Iceland|\n",
      "|  South Korea|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Country').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cbe894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|Joachim|\n",
      "|  Diana|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df.where(\"Country == 'Brazil'\").select('Name').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c110483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|      Country|sum(Price)|\n",
      "+-------------+----------+\n",
      "|       Sweden|      8400|\n",
      "|       Jersey|      1200|\n",
      "|     Malaysia|      1200|\n",
      "|       Turkey|      2400|\n",
      "|      Germany|     22800|\n",
      "|       France|     30300|\n",
      "|      Belgium|      3600|\n",
      "|      Finland|      1200|\n",
      "|United States|    350350|\n",
      "|        India|      2400|\n",
      "|       Kuwait|      1200|\n",
      "|        Malta|      3600|\n",
      "|        Italy|      2400|\n",
      "|       Norway|     12000|\n",
      "|        Spain|      2400|\n",
      "|      Denmark|      8400|\n",
      "|      Ireland|     29100|\n",
      "|       Israel|      1200|\n",
      "|      Iceland|      1200|\n",
      "|  South Korea|      1200|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").sum(\"Price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7079dbd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             Country|sum(Price)|\n",
      "+--------------------+----------+\n",
      "|       United States|    350350|\n",
      "|      United Kingdom|     63600|\n",
      "|              Canada|     42000|\n",
      "|              France|     30300|\n",
      "|             Ireland|     29100|\n",
      "|           Australia|     22800|\n",
      "|             Germany|     22800|\n",
      "|         Switzerland|     19200|\n",
      "|         Netherlands|     14400|\n",
      "|              Norway|     12000|\n",
      "|              Brazil|      8700|\n",
      "|              Sweden|      8400|\n",
      "|             Denmark|      8400|\n",
      "|               Malta|      3600|\n",
      "|United Arab Emirates|      3600|\n",
      "|             Austria|      3600|\n",
      "|             Belgium|      3600|\n",
      "|        South Africa|      3600|\n",
      "|              Turkey|      2400|\n",
      "|               India|      2400|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").sum(\"Price\").orderBy(\"sum(Price)\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4bfd4b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('../data/countries.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6a92f41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID|sum(Price)|\n",
      "+---+----------+\n",
      "|  2|    350350|\n",
      "|  1|     63600|\n",
      "|  8|     42000|\n",
      "|  5|     30300|\n",
      "|  7|     29100|\n",
      "| 19|     22800|\n",
      "|  3|     22800|\n",
      "| 12|     19200|\n",
      "|  6|     14400|\n",
      "| 16|     12000|\n",
      "| 33|      8700|\n",
      "| 13|      8400|\n",
      "| 15|      8400|\n",
      "| 26|      3600|\n",
      "| 22|      3600|\n",
      "| 10|      3600|\n",
      "| 28|      3600|\n",
      "| 14|      3600|\n",
      "| 34|      2400|\n",
      "|  9|      2400|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2,\"Country\").select(\"ID\", \"Price\").groupBy(\"ID\").sum(\"Price\").orderBy(\"sum(Price)\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fb841f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"Name\").count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9689cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|src|              rank|\n",
      "+---+------------------+\n",
      "|  3|0.9999999999999998|\n",
      "|  1|1.2983805334459568|\n",
      "|  4|0.9999999999999998|\n",
      "|  2|0.7016194665540426|\n",
      "+---+------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1931.showString.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:86)\n\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:116)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$StringConcat.toString(StringUtils.scala:147)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:174)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2275/0x0000000840f9d040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2265/0x0000000840f97040.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23580/3465504402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rank'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/spark/spark-3.3.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/spark/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/spark/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/spark/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1931.showString.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:86)\n\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:116)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$StringConcat.toString(StringUtils.scala:147)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:174)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2275/0x0000000840f9d040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2265/0x0000000840f97040.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "lines = spark.read.text(\"../data/pagerank_data.txt\")\n",
    "# You can also test your program on the follow larger data set:\n",
    "# lines = spark.read.text(\"../data/dblp.in\")\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "# print(a.show())\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "# print(links.show())\n",
    "outdegrees = links.groupBy('src').count()\n",
    "# print(outdegrees.show())\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank'))\n",
    "# tmp1 = ranks.withColumnRenamed('src', 'dst').join(links, 'dst').groupBy('src').count()  \n",
    "# tmp2 = links.join(ranks, 'src')\n",
    "# tmp3 = tmp2.join(tmp1, 'src')\n",
    "# tmp3 = tmp3.withColumn('new', (col('rank')/(col('count'))))\n",
    "# tmp3 = tmp3.select(\"dst\",'new').groupBy('dst').agg(sum(\"new\").alias('rank'))\n",
    "# tmp3 = tmp3.withColumn('rank', col('rank') * 0.85 + 0.15).withColumnRenamed('dst', 'src')\n",
    "\n",
    "# tmp3.withColumn(\"newRank\",\\\n",
    "#     (col('rank')/col('count')))\\\n",
    "#     .groupBy('dst')\\\n",
    "#     .sum('newRank')\\\n",
    "#     .withColumnRenamed('sum(newRank)', 'rank')\\\n",
    "#     .withColumnRenamed('dst', 'src')\\\n",
    "#     .withColumn('rank', col('rank') * 0.85 + 0.15)\n",
    "\n",
    "for iteration in range(numOfIterations):\n",
    "# # # # FILL IN THIS PART\n",
    "    tmp1 = ranks.select(col('src').alias('dst')).join(links, 'dst').groupBy('src').count().persist()\n",
    "    tmp2 = links.join(ranks, 'src').persist()\n",
    "    tmp3 = tmp2.join(tmp1, 'src')\n",
    "    tmp3 = tmp3.select('dst',((col(\"rank\")/col(\"count\"))).alias(\"new\"))\n",
    "    tmp3 = tmp3.select(\"dst\",'new').groupBy('dst').agg(sum(\"new\").alias('rank'))\n",
    "    ranks = tmp3.select(col('dst').alias('src'), (tmp3.rank*0.85 + 0.15).alias('rank'))\n",
    "    if iteration > 7:\n",
    "        ranks.cache()\n",
    "        ranks.show()\n",
    "\n",
    "ranks.orderBy(desc('rank')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
